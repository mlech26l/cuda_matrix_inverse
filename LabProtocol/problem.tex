% !TEX root =  root.tex


\section{PROBLEM STATEMENT}\label{sec:problem}
First we will repeat some mathematical definitions for the sake of completeness.
\begin{definition}
	\textbf{Inverse Matrix}
	Let $A \in \mathbb{R}^n \times \mathbb{R}^n$ and $I_n$ be the identity matrix of dimension $n$, then the matrix $A^{-1} \in \mathbb{R}^n \times \mathbb{R}^n$ is called \textbf{inverse matrix} of $A$ if and only if
	\begin{equation}\label{eq:inverse}
	A \cdot A^{-1} = I_n
	\end{equation}
\end{definition}
A quadratic matrix $A$ is called \emph{regular} if there exists a inverse matrix of $A$. If there exists no inverse of $A$ then $A$ is called \emph{singular}.\\
Inverse matrices can be though to find, but have various of different uses especially in control theory to describe linear systems.  
\vspace{0.2cm}\\
There are a lot of ways to compute the inverse of a matrix, we will introduce some of the most common ones.\\
\subsection*{Gaussian-Elimination}
Calculcating the inverse by Gaussian-elemination is done by starting with two matrixes, the matrix A that should be inverted and matrix B which is initialized to the identity matrix. By performing row reductions on matrix A to make it into the identity matrix and copying all operations to matrix B gives that B is the inverse of A when A is the identity matrix.

The operations of transforming A into the identity matrix is divided into steps, forward elimination and backward elimination.
While doing forward elimination, row operations is performed on the matrix so that the diagonal is 1 and that every element below is zero.
Backward elimination is performing row operations so that all elements above the diagonal is zero.

\subsection*{Inversion by adjugate matrix}
The inverse $A^{-1}$ of $A$ can be computed by:
\begin{equation}
	A^{-1}_{i,j} := \frac{1}{\text{det}(A)} \text{adj}_{i,j}(A)
\end{equation}
with
\begin{definition}
	\textbf{Adjugate matrix} Let $M_{i,j}$ be the matrix $A$ where the $i$-th row and the $j$-th column are removed. Then:
	\[ \text{\textnormal{adj}}_{i,j}(A) = (-1)^{i+j} \text{\textnormal{det}}(M_{j,i}) \]
\end{definition}

This way of inverting a matrix works well for small $n$ (especially if $n \leq 3$), because the determinant is easy to compute in these cases.\\

For bigger matrices we have to deal with calculating $n^2$ determinants for $n-1$ matrices. There are a lot of good algorithms \cite{Dodgson1866} and \cite{ABDELMALEK2007} to compute determinants on a Parallel architecture. To run the algorithm for one determinant we need on average $n$ kernel launches. Therefore we get $n^3$ kernel launches. We could also compute all determinants in parallel, but it will need $n^4$ of memory. So for instances with size bigger then 1000 this method is not handleable. \\

The matrices $M_{j,i}$ are very similar to each other. This can be used to save a lot of memory and computation time. But we did not find any algorithm which utilize this fact.\\

Over all the inversion method by adjugate matrix is not suitable for bigger matrices.  Therefore we did not implement it.

\subsection*{LU decomposition}
The LU decomposition, namely with Crout's method, computes two matrices $L$ and $U$, such that $A=L \cdot U$. Where $L$ is a lower triangular matrix and $U$ is a upper triangular with 1's in the diagonal. An example with dimension 3 is:
\begin{align*}
	 \begin{pmatrix}
	 a_{11} & a_{12} & a_{13}\\
	 a_{21} & a_{22} & a_{23}\\
	 a_{31} & a_{32} & a_{33}\\
	 \end{pmatrix} & = \begin{pmatrix} l_{11}  & 0 & 0\\
	 l_{21} & l_{22} & 0\\
	 l_{31} & l_{32} & l_{33}
	 \end{pmatrix} \cdot \begin{pmatrix} 1 & u_{12} & u_{13}\\
	 0 & 1 & u_{23}\\
	 0 & 0 & 1\\
	 \end{pmatrix}\\
	& = \begin{pmatrix} l_{11} & l_{11} u_{12} & l_{11} u_{13}\\
	l_{21} & l_{21} u_{12} + l_{22} & l_{21} u_{13} + l_{22} u_{23}\\
	l_{31} & l_{31} u_{12} + l_{32} & l_{31} u_{13} + l_{32} u_{23} + l_{33}\\
	\end{pmatrix}
\end{align*} 
From this equation we will derive a recursive algorithm for computing the $L$ and $U$ matrices:
First we will set $l_{11} = a_{11}$, next we will dived the first row of $A$ by $l_{11}$, thus giving us $u_{12}$ and $u_{13}$, i.e. the first row of $U$.\\
Next we will set the remaining first column of $L$ to the first column of $A$, i.e. $l_{k,1} = a_{k,1}$. Finally we will update the right bottom 2 by 2 sub matrix by subtracting the product of the corresponding elements from the first row. I.e. the remaining 2-by-2 matrix will be of the form: 
\begin{equation*}
 \begin{pmatrix}
l_{22} & l_{22} u_{23}\\
l_{32} & l{32} u_{23} + l_{32}\\
\end{pmatrix}
\end{equation*}
Thus we can apply the same algorithm recursively on this sub-matrix.
\vspace{0.3cm}\\
During this process we divided by $l_{11}$, so in the scenario where $l_{11}=0$, it is necessary to swap the first row with a different row where the first element is not zero. Formally defined this corresponds to a row-permutation of the matrix $A$. So in fact we are looking for the two matrices $L$, $U$ and a row-permutation $\pi$, such that $\pi(A) = L \cdot U$\\  
If there is no such row, then the matrix $A$ is singular, i.e. there exists to LU decomposition and no inverse. \\
For keeping the numerical error done by this process low, it is recommended to find a row-permutation $\pi$, such that in each step the element $l_{11}$ is maximal with respect to the first column.
\vspace{0.3cm}\\
To get $A^{-1}$, every column of the inverse is computed separately, i.e. we need to find column vectors $x_i$ such that $L \cdot U \cdot x = b_i$, where $b_i$ is the i-th column of the identity matrix. Note that if we use a row-permutation $\pi$, then $\pi$ must also be applied to the vector $b_i$. The computation is again separated into two steps:\\
\begin{enumerate}
	\item Find vectors $y_i$ such that $L \cdot y_i = b_i$, this is called forward substitution
	\item Find vectors $x_i$ such that $U \cdot x_i = y_i$, this is called backward substitution
\end{enumerate}
With our example:
\begin{align*}
	\begin{pmatrix} l_{11}  & 0 & 0\\
	l_{21} & l_{22} & 0\\
	l_{31} & l_{32} & l_{33}
	\end{pmatrix} \cdot \begin{pmatrix} y_{i,1} \\ y_{i,2} \\ y_{i,3}	\end{pmatrix}
	& = \begin{pmatrix}
	b_{i,1} \\ b_{i,2} \\ b_{i,3}
	\end{pmatrix} \\
	\begin{pmatrix}
	l_{11} y_{i,1}\\ l_{21} y_{i,1} + l_{22} y_{i,2}\\
	l_{31} y_{i,1} + l_{32} y_{i,2} + l_{33} y_{i,3}
	\end{pmatrix}	& = \begin{pmatrix}
	b_{i,1} \\ b_{i,2} \\ b_{i,3}
	\end{pmatrix} \\
\end{align*}
Where we will start to compute the top most element $y_{i,1}$ first and iteratively computing the next element down to the bottom most element, this is why this process is called forward substitution.
\vspace{0.3cm}\\
Having computed $y_i$ we will next compute $x_i$, e.g.
\begin{align*}
\begin{pmatrix} 1 & u_{12} & u_{13}\\
0 & 1 & u_{23}\\
0 & 0 & 1
\end{pmatrix} \cdot \begin{pmatrix} x_{i,1} \\ x_{i,2} \\ x_{i,3}	\end{pmatrix}
& = \begin{pmatrix} y_{i,1} \\ y_{i,2} \\ y_{i,3}	\end{pmatrix}\\
\begin{pmatrix}
x_{i,1} + x_{i,2} u_{12} + x_{i,3} u_{13}\\ x_{i,2} + x_{i,3} u_{23}\\
x_{i,3}
\end{pmatrix}	& = \begin{pmatrix} y_{i,1} \\ y_{i,2} \\ y_{i,3}	\end{pmatrix}\\
\end{align*}
As the name backwards substitution reveals, this time we will start computing the bottom most element $x_{i,3}$ first and iteratively use the results to compute the second bottom most element and so on.

\subsection*{CUDA terminology}
In the following the term \emph{host} refers to program code that is executed on the CPU. On the other hand is the term \emph{device} used to refer to program code that is executed on the GPU or computation card.\\
A \emph{kernel} is a function that can be executed on the device.

